{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralelização e Distribuição do Processamento com MPI\n",
    "\n",
    "## Geração do grafo\n",
    "\n",
    "Ambos os grafos a serem utilizados serão os mesmos das abordagens anteriores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código com MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MPI1.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile MPI1.cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <fstream>\n",
    "#include <algorithm>\n",
    "#include <mpi.h>\n",
    "#include <chrono>\n",
    "\n",
    "using namespace std;\n",
    "using namespace std::chrono;\n",
    "\n",
    "vector<vector<int>> LerGrafo(const string &nomeArquivo, int &numVertices) {\n",
    "    ifstream arquivo(nomeArquivo);\n",
    "    int numArestas;\n",
    "    arquivo >> numVertices >> numArestas;\n",
    "    vector<vector<int>> grafo(numVertices, vector<int>(numVertices, 0));\n",
    "    for (int i = 0; i < numArestas; ++i) {\n",
    "        int u, v;\n",
    "        arquivo >> u >> v;\n",
    "        grafo[u - 1][v - 1] = 1;\n",
    "        grafo[v - 1][u - 1] = 1;\n",
    "    }\n",
    "    arquivo.close();\n",
    "    return grafo;\n",
    "}\n",
    "\n",
    "bool VerificaClique(const vector<vector<int>> &grafo, const vector<int> &clique) {\n",
    "    for (int i = 0; i < clique.size(); i++) {\n",
    "        for (int j = i + 1; j < clique.size(); j++) {\n",
    "            if (grafo[clique[i]][clique[j]] == 0) { return false; }\n",
    "        }\n",
    "    }\n",
    "    return true;\n",
    "}\n",
    "\n",
    "void EncontrarCliquesLocais(const vector<vector<int>>& grafo, const vector<int>& vertices_Iniciais, vector<vector<int>>& cliques_Locais) {\n",
    "    vector<int> atualClique;\n",
    "\n",
    "    auto backtrack = [&](int v, auto& backtrack_ref) -> void {\n",
    "        // Salva o clique atual como uma solução válida\n",
    "        if (!atualClique.empty()) { cliques_Locais.push_back(atualClique); }\n",
    "        for (int i = v; i < grafo.size(); i++) {\n",
    "            // Verifica se o vértice pode ser adicionado ao clique atual\n",
    "            bool adiciona = true;\n",
    "            for (int u : atualClique) {\n",
    "                if (grafo[u][i] == 0) { // Não é clique\n",
    "                    adiciona = false;\n",
    "                    break;\n",
    "                }\n",
    "            }\n",
    "            if (adiciona) {\n",
    "                atualClique.push_back(i);\n",
    "                backtrack_ref(i + 1, backtrack_ref);\n",
    "                atualClique.pop_back();\n",
    "            }\n",
    "        }\n",
    "    };\n",
    "    // Pra cada vértice inicial, inicia a busca de cliques\n",
    "    for (int vInicial : vertices_Iniciais) {\n",
    "        atualClique.clear();\n",
    "        atualClique.push_back(vInicial);\n",
    "        backtrack(vInicial + 1, backtrack);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    int rank, size;\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "    high_resolution_clock::time_point inicio, fim;\n",
    "\n",
    "    int numVertices = 0;\n",
    "    vector<vector<int>> grafo;\n",
    "\n",
    "    if (rank == 0) {\n",
    "        grafo = LerGrafo(\"grafo1.txt\", numVertices);\n",
    "        \n",
    "        inicio = high_resolution_clock::now();\n",
    "        \n",
    "        // Enviando numVertices para os processos\n",
    "        for (int i = 1; i < size; i++) { MPI_Send(&numVertices, 1, MPI_INT, i, 0, MPI_COMM_WORLD); }\n",
    "\n",
    "        // Enviado o grafo para os processos\n",
    "        for (int i = 1; i < size; i++) {\n",
    "            for (int j = 0; j < numVertices; j++) { MPI_Send(grafo[j].data(), numVertices, MPI_INT, i, 0, MPI_COMM_WORLD); }\n",
    "        }\n",
    "    } else {\n",
    "        // Receber o numVertices do processo raiz\n",
    "        MPI_Recv(&numVertices, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        grafo.resize(numVertices, vector<int>(numVertices));\n",
    "        // Receber o grafo do processo raiz\n",
    "        for (int j = 0; j < numVertices; j++) { MPI_Recv(grafo[j].data(), numVertices, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); }\n",
    "    }\n",
    "\n",
    "    // Dividir\n",
    "    vector<int> verticesIniciais;\n",
    "    for (int i = rank; i < numVertices; i += size) { verticesIniciais.push_back(i); }\n",
    "\n",
    "    // Cada processo executa a busca de cliques\n",
    "    vector<vector<int>> Locais;\n",
    "    EncontrarCliquesLocais(grafo, verticesIniciais, Locais);\n",
    "\n",
    "    if (rank == 0) {\n",
    "        vector<vector<int>> Cliques;\n",
    "        Cliques.insert(Cliques.end(), Locais.begin(), Locais.end());\n",
    "\n",
    "        for (int i = 1; i < size; i++) {\n",
    "            int tam;\n",
    "            MPI_Recv(&tam, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "            vector<int> recebidos(tam);\n",
    "            MPI_Recv(recebidos.data(), tam, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "\n",
    "            for (int k = 0; k < tam; k) {\n",
    "                int tamClique = recebidos[k++];\n",
    "                vector<int> clique;\n",
    "                for (int j = 0; j < tamClique; j++) { clique.push_back(recebidos[k++]); }\n",
    "                Cliques.push_back(clique);\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        size_t maior = 0;\n",
    "        for (const auto& clique : Cliques) {\n",
    "            if (clique.size() > maior) { maior = clique.size(); }  // Maior tamanho de clique\n",
    "        }\n",
    "        \n",
    "        vector<int> cliquesMax;\n",
    "        for (const auto& clique : Cliques) { // Primeira clique de maior tamanho encontrada\n",
    "            if (clique.size() == maior) { cliquesMax = clique; }\n",
    "        }\n",
    "\n",
    "        fim = high_resolution_clock::now();\n",
    "        duration<double> duracao = duration_cast<duration<double>>(fim - inicio);\n",
    "\n",
    "        cout << \"Clique máxima arquivo saída: \";\n",
    "        for (int m = 0; m < cliquesMax.size(); m++) { cout << cliquesMax[m]+1 << \" \"; }\n",
    "        cout << endl;\n",
    "        cout << \"Tempo de execução: \" << duracao.count() << \" segundos\" << endl;\n",
    "    }\n",
    "    else {\n",
    "        vector<int> enviar;\n",
    "        for (const auto& clique : Locais) {\n",
    "            enviar.push_back(clique.size());\n",
    "            enviar.insert(enviar.end(), clique.begin(), clique.end());\n",
    "        }\n",
    "\n",
    "        // Enviar o tamanho e os dados\n",
    "        int tam = enviar.size();\n",
    "        MPI_Send(&tam, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n",
    "        MPI_Send(enviar.data(), tam, MPI_INT, 0, 0, MPI_COMM_WORLD);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!mpic++ -o MPI MPI.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpic++ -o MPI1 MPI1.cpp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando arquivo de configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submit1.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile submit1.slurm\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=mpi                          # Nome do job\n",
    "#SBATCH --output=mpi_%j.txt                     # Nome do arquivo de saída\n",
    "#SBATCH --ntasks=4                              # Número de tarefas \n",
    "#SBATCH --mem=2048                               # Memória total alocada por nó\n",
    "#SBATCH --time=00:20:00                         # Tempo máximo de execução\n",
    "#SBATCH --partition=espec                      # fila do cluster a ser utilizada\n",
    "\n",
    "mpirun ./MPI1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10023\n"
     ]
    }
   ],
   "source": [
    "!sbatch submit.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10024\n"
     ]
    }
   ],
   "source": [
    "!sbatch submit1.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando a verificação do grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 12, 14, 20, 30, 32, 33, 38, 43, 47, 49]\n",
      "['Clique máxima arquivo saída: 3 8 19 23 28 31 32 33 40 43 50 \\n', 'Tempo de execução: 0.0704077 segundos\\n']\n",
      "[10, 11, 12, 15, 30, 32, 35, 41, 47, 58, 60, 61, 68, 91, 100]\n",
      "['Clique máxima arquivo saída: 10 11 12 15 30 32 35 41 47 58 60 61 68 91 100 \\n', 'Tempo de execução: 9.90859 segundos\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('verificacao.txt', 'r') as file:\n",
    "    lines = file.readlines()  # Lê todas as linhas\n",
    "    ultima_linha = lines[-1]  # Pega a última linha\n",
    "    _, numeros = ultima_linha.split(\"=\", 1)\n",
    "    lista = list(map(int, numeros.split()))\n",
    "    lista.sort()\n",
    "    print(lista)\n",
    "\n",
    "with open('mpi_10023.txt', 'r') as file:\n",
    "    lines = file.readlines()  # Lê todas as linhas\n",
    "    print(lines)\n",
    "\n",
    "with open('verificacao1.txt', 'r') as file:\n",
    "    lines = file.readlines()  # Lê todas as linhas\n",
    "    ultima_linha = lines[-1]  # Pega a última linha\n",
    "    _, numeros = ultima_linha.split(\"=\", 1)\n",
    "    lista = list(map(int, numeros.split()))\n",
    "    lista.sort()\n",
    "    print(lista)\n",
    "\n",
    "with open('mpi_10024.txt', 'r') as file:\n",
    "    lines = file.readlines()  # Lê todas as linhas\n",
    "    print(lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tempo\n",
    "\n",
    "Com a implementação MPI realizada, afirma-se, baseando-se na Tabela a seguir, que apesar de não ter sido mais rápido que a implementação em OpenMP, ainda foi mais rápido que a exaustiva.\n",
    "\n",
    "| Implementação | numVertices |   Tempo     |\n",
    "|---------------|-------------|-------------|\n",
    "| exaustiva     |     50      | 0.193033 s  |\n",
    "| exaustiva     |     100     | 51.3523 s   |\n",
    "| openMP        |     50      | 0.0220482 s |\n",
    "| openMP        |     100     | 3.59543 s   |\n",
    "| MPI           |     50      | 0.0704077 s |\n",
    "| MPI           |     100     | 9.90859 s   |\n",
    "\n",
    "O speed up resultante é:\n",
    "\n",
    "| Implementação | numVertices | speed up |\n",
    "|---------------|-------------|----------|\n",
    "| openMP        |     50      |  8.76    |\n",
    "| openMP        |     100     | 14.29    |\n",
    "| MPI           |     50      |  2.74    |\n",
    "| MPI           |     100     |  5.18    |\n",
    "\n",
    "Logo, o MPI também acelerou a execução, mas com menor eficiência comparado ao OpenMP.\n",
    "\n",
    "O OpenMP pode ter sido mais rápido que o MPI porque a tarefa pode ser mais eficiente em sistemas de memória compartilhada, onde os threads podem acessar rapidamente os dados e se comunicar com menos latência. Já o MPI sofre mais com a sobrecarga de comunicação, especialmente se a aplicação não for distribuída de maneira ideal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
